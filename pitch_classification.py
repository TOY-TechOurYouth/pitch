import os
import librosa
import numpy as np
import pandas as pd
import joblib
from tqdm import tqdm
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
import seaborn as sns

# === í•œê¸€ í°íŠ¸ ì„¤ì • (Windows) ===
plt.rcParams["font.family"] = "Malgun Gothic"
plt.rcParams["axes.unicode_minus"] = False

# === ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì • ===
RESULT_DIR = r"C:\Users\user\PycharmProjects\Toy2\pitch_result"
os.makedirs(RESULT_DIR, exist_ok=True)

# === ê²½ë¡œ ì„¤ì • ===
CSV_PATH = r"C:\Users\user\PycharmProjects\TOY\gender_only\combined_pitch_data.csv"
AUDIO_DIRS = [
    r"C:\Users\user\PycharmProjects\TOY\four",
    r"C:\Users\user\PycharmProjects\TOY\five",
    r"C:\Users\user\PycharmProjects\TOY\five_2"
]
BEST_MODEL_PATH = r"C:\Users\user\PycharmProjects\Toy2\CNN_model_result\best_model.h5"
SCALER_PATH = r"/files_for_train/scaler.pkl"
ENCODER_PATH = r"/files_for_train/label_encoder.pkl"

SR = 16000 # ìƒ˜í”Œë§ ë ˆì´íŠ¸
DURATION = 1.0 # 1ì´ˆ ë‹¨ìœ„ ì²­í¬
HYBRID_FEATURE_DIM = 175 # ì¶”ì¶œë  feature ë²¡í„°ì˜ ê¸¸ì´

PITCH_THRESHOLDS = {
    'male': [85, 155, 180],
    'female': [165, 230, 300]
}

# === í•¨ìˆ˜ ì •ì˜ ===

# wav_idì— í•´ë‹¹í•˜ëŠ” .wav íŒŒì¼ì„ ì—¬ëŸ¬ í´ë”ì—ì„œ ì°¾ì•„ ê²½ë¡œë¥¼ ë°˜í™˜
def find_wav_path(wav_id):
    for base_dir in AUDIO_DIRS:
        candidate = os.path.join(base_dir, wav_id + ".wav")
        if os.path.exists(candidate):
            return candidate, os.path.basename(base_dir)
    return None, None

# ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ì„¸ê°œ í”¼ì²˜ ì¶”ì¶œí•˜ì—¬ 1D ë²¡í„°ë¡œ ì—°ê²°
def extract_hybrid_feature(file_path):
    try:
        y, sr = librosa.load(file_path, sr=None)
        y, _ = librosa.effects.trim(y)
        if len(y) < int(sr * DURATION):
            y = np.pad(y, (0, int(sr * DURATION) - len(y)))
        else:
            y = y[:int(sr * DURATION)]

        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
        contrast = librosa.feature.spectral_contrast(y=y, sr=sr)

        return np.concatenate([
            mfcc.mean(axis=1),
            mel.mean(axis=1),
            contrast.mean(axis=1)
        ])
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ (feature): {file_path} - {e}")
        return None

# librosaì˜ YIN ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•´ í‰ê·  í”¼ì¹˜ ì¶”ì¶œ
def extract_pitch(y, sr=16000):
    pitches = librosa.yin(y, fmin=50, fmax=500, sr=sr)
    pitches = pitches[~np.isnan(pitches)]
    return np.mean(pitches) if len(pitches) > 0 else None

# í‰ê·  í”¼ì¹˜ë¥¼ ë‚¨/ì—¬ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜
def classify_pitch(avg_pitch, gender):
    if gender not in PITCH_THRESHOLDS or avg_pitch is None:
        return "Unclear"
    low, mid, high = PITCH_THRESHOLDS[gender]
    if avg_pitch < mid:
        return "ë‚®ìŒ"
    elif avg_pitch < high:
        return "ì¤‘ê°„"
    else:
        return "ë†’ìŒ"

# CNN ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ì˜ confidenceê°€ íŠ¹ì • thresholdë³´ë‹¤ ë‚®ìœ¼ë©´ "unknown" ì²˜ë¦¬
def predict_with_uncertainty(model, sample_vector, label_encoder, threshold=0.75):
    if sample_vector.ndim == 1:
        sample_vector = sample_vector[..., np.newaxis]
    sample_vector = np.expand_dims(sample_vector, axis=0)
    probs = model.predict(sample_vector, verbose=0)[0]
    confidence = np.max(probs)
    predicted_index = np.argmax(probs)
    if confidence < threshold:
        return "unknown", probs
    else:
        return label_encoder.inverse_transform([predicted_index])[0], probs

# ì „ì²´ ì˜¤ë””ì˜¤ë¥¼ 1ì´ˆ ë‹¨ìœ„ë¡œ ìŠ¬ë¼ì´ì‹±
def split_audio_to_chunks(y, sr, chunk_duration=1.0):
    chunk_len = int(sr * chunk_duration)
    return [y[i:i + chunk_len] for i in range(0, len(y), chunk_len) if len(y[i:i + chunk_len]) == chunk_len]

# === ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ===
df = pd.read_csv(CSV_PATH)
paths, sources = [], []
for wav_id in df["wav_id"]:
    path, source = find_wav_path(wav_id)
    paths.append(path)
    sources.append(source)

df["file_path"] = paths
df["source"] = sources
df = df.dropna(subset=["file_path"])

# === ëª¨ë¸ ë¡œë“œ ===
best_model = load_model(BEST_MODEL_PATH)
scaler = joblib.load(SCALER_PATH)
label_encoder = joblib.load(ENCODER_PATH)

# === ì˜ˆì¸¡ ë° ì €ì¥ ===
results = []
for i, row in tqdm(df.iterrows(), total=len(df)):
    try:
        file_path = row["file_path"]
        wav_id = row["wav_id"]
        source = row["source"]

        # [1] Feature
        feature_vector = extract_hybrid_feature(file_path)
        if feature_vector is None or feature_vector.shape[0] != HYBRID_FEATURE_DIM:
            raise ValueError("Invalid feature shape")

        # ìŠ¤ì¼€ì¼ë§(ì •ê·œí™”)
        feature_scaled = scaler.transform([feature_vector])[0]

        # íšŒìƒ‰ì§€ëŒ€ ì¶”ì¶œ
        predicted_gender, probs = predict_with_uncertainty(best_model, feature_scaled, label_encoder)

        # 1ì´ˆ ë‹¨ìœ„ pitch ë¶„ë¥˜
        y, _ = librosa.load(file_path, sr=SR)
        y, _ = librosa.effects.trim(y)
        chunks = split_audio_to_chunks(y, sr=SR, chunk_duration=1.0)

        for idx, chunk in enumerate(chunks):
            pitch = extract_pitch(chunk, sr=SR)
            pitch_category = "ì¤‘ê°„" if predicted_gender == "unknown" else classify_pitch(pitch, predicted_gender)
            results.append((wav_id, source, idx + 1, predicted_gender, pitch, pitch_category))

    except Exception as e:
        results.append((row["wav_id"], "ì˜¤ë¥˜", -1, "ì˜¤ë¥˜", 0, "Unclear"))

# === ê²°ê³¼ ì €ì¥ ===
final_df = pd.DataFrame(results, columns=["wav_id", "source", "chunk_index", "ì˜ˆì¸¡ ì„±ë³„", "í‰ê·  í”¼ì¹˜ (Hz)", "í”¼ì¹˜ ì¹´í…Œê³ ë¦¬"])
csv_path = os.path.join(RESULT_DIR, "final_gender_pitch_result_chunks.csv")
final_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
print(f"âœ… ìµœì¢… CSV ì €ì¥ ì™„ë£Œ: {csv_path}")

# === í†µê³„ ì¶œë ¥ ===
print("\nğŸ“Š ì˜ˆì¸¡ ì„±ë³„ ë¶„í¬:")
print(final_df["ì˜ˆì¸¡ ì„±ë³„"].value_counts())
print("\nğŸ“Š í”¼ì¹˜ ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
print(final_df["í”¼ì¹˜ ì¹´í…Œê³ ë¦¬"].value_counts())

# === ğŸ“Š ì‹œê°í™” 1: íˆíŠ¸ë§µ ===
plt.figure(figsize=(8, 5))
cross_tab = pd.crosstab(final_df["ì˜ˆì¸¡ ì„±ë³„"], final_df["í”¼ì¹˜ ì¹´í…Œê³ ë¦¬"])
sns.heatmap(cross_tab, annot=True, fmt="d", cmap="Blues")
plt.title("ì„±ë³„ vs í”¼ì¹˜ ì¹´í…Œê³ ë¦¬ ë¶„í¬ íˆíŠ¸ë§µ")
plt.xlabel("í”¼ì¹˜ ì¹´í…Œê³ ë¦¬")
plt.ylabel("ì˜ˆì¸¡ ì„±ë³„")
plt.tight_layout()
heatmap_path = os.path.join(RESULT_DIR, "heatmap_gender_pitch.png")
plt.savefig(heatmap_path)

# === ğŸ“Š ì‹œê°í™” 2: ë°•ìŠ¤í”Œë¡¯ ===
plt.figure(figsize=(8, 5))
sns.boxplot(x="ì˜ˆì¸¡ ì„±ë³„", y="í‰ê·  í”¼ì¹˜ (Hz)", data=final_df)
plt.title("ì˜ˆì¸¡ ì„±ë³„ë³„ í‰ê·  í”¼ì¹˜ ë°•ìŠ¤í”Œë¡¯")
plt.ylabel("í‰ê·  í”¼ì¹˜ (Hz)")
plt.tight_layout()
boxplot_path = os.path.join(RESULT_DIR, "boxplot_pitch_by_gender.png")
plt.savefig(boxplot_path)
